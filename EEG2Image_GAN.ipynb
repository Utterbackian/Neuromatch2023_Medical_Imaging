{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utterbackian/Neuromatch2023_Medical_Imaging/blob/main/EEG2Image_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQr2dUBrhsCt"
      },
      "source": [
        "GAN Model from EEG2Image Paper converted to Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAip865JhdK1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import spectral_norm\n",
        "import torchvision.transforms.functional as TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnjPOG1YhRDp"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, n_class=10, res=128):\n",
        "        super(Generator, self).__init__()\n",
        "        filters = [1024, 512, 256, 128, 64, 32]  # , 16]\n",
        "        strides = [4, 2, 2, 2, 2, 2]  # , 2]\n",
        "\n",
        "        self.cnn_depth = len(filters)\n",
        "\n",
        "        # For discrete condition we are using Embedding\n",
        "        self.cond_embedding = nn.Embedding(num_embeddings=n_class, embedding_dim=50)\n",
        "        self.cond_flat = nn.Flatten()\n",
        "        self.cond_dense = nn.Linear(in_features=8 * 8 * 1, out_features=64)\n",
        "        self.cond_reshape = nn.Reshape((64,))\n",
        "\n",
        "        # Hyperparameter:\n",
        "        # If only conv  : mean=0.0, var=0.02\n",
        "        # If using bnorm: mean=1.0, var=0.02\n",
        "        self.conv = nn.ModuleList([\n",
        "            spectral_norm(nn.ConvTranspose2d(\n",
        "                in_channels=1, out_channels=filters[idx], kernel_size=3,\n",
        "                stride=strides[idx], padding=1, bias=False))\n",
        "            for idx in range(self.cnn_depth)\n",
        "        ])\n",
        "\n",
        "        self.act = nn.ModuleList([nn.LeakyReLU() for idx in range(self.cnn_depth)])\n",
        "        self.bnorm = nn.ModuleList([nn.BatchNorm2d(filters[idx]) for idx in range(self.cnn_depth)])\n",
        "\n",
        "        self.last_conv = spectral_norm(nn.Conv2d(\n",
        "            in_channels=filters[-1], out_channels=3, kernel_size=3,\n",
        "            stride=1, padding=1, bias=False))\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = X.unsqueeze(2).unsqueeze(3)\n",
        "        X = self.act[0](self.conv[0](X))\n",
        "\n",
        "        for idx in range(1, self.cnn_depth):\n",
        "            X = self.act[idx](self.bnorm[idx](self.conv[idx](X)))\n",
        "        X = self.last_conv(X)\n",
        "        return X\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_class=10, res=128):\n",
        "        super(Discriminator, self).__init__()\n",
        "        filters = [64, 128, 256, 512, 1024, 1]\n",
        "        strides = [2, 2, 2, 2, 1, 1]\n",
        "        self.cnn_depth = len(filters)\n",
        "\n",
        "        self.cond_embedding = nn.Embedding(num_embeddings=n_class, embedding_dim=50)\n",
        "        self.cond_flat = nn.Flatten()\n",
        "        self.cond_dense = nn.Linear(in_features=res * res * 1, out_features=res * res * 1)\n",
        "        self.cond_reshape = nn.Reshape((res, res, 1))\n",
        "\n",
        "        self.cnn_conv = nn.ModuleList([\n",
        "            spectral_norm(nn.Conv2d(\n",
        "                in_channels=1, out_channels=filters[i], kernel_size=3,\n",
        "                stride=strides[i], padding=1, bias=False))\n",
        "            for i in range(self.cnn_depth)\n",
        "        ])\n",
        "\n",
        "        self.cnn_bnorm = nn.ModuleList([nn.BatchNorm2d(filters[i]) for i in range(self.cnn_depth)])\n",
        "        self.cnn_act = nn.ModuleList([nn.LeakyReLU(negative_slope=0.2) for i in range(self.cnn_depth)])\n",
        "\n",
        "        self.flat = nn.Flatten()\n",
        "        self.disc_out = nn.Linear(in_features=res * res * 1, out_features=1)\n",
        "\n",
        "    def forward(self, x, C):\n",
        "        C = C.unsqueeze(2).unsqueeze(3)\n",
        "        C = C.expand(-1, x.shape[1], x.shape[2], -1)\n",
        "        x = torch.cat([x, C], dim=-1)\n",
        "\n",
        "        for layer_no in range(self.cnn_depth):\n",
        "            x = self.cnn_act[layer_no](self.cnn_bnorm[layer_no](self.cnn_conv[layer_no](x)))\n",
        "\n",
        "        reconst_x = None\n",
        "        x = self.disc_out(self.flat(x))\n",
        "\n",
        "        return x, reconst_x\n",
        "\n",
        "class DCGAN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DCGAN, self).__init__()\n",
        "        self.gen = Generator()\n",
        "        self.disc = Discriminator()\n",
        "\n",
        "    def forward(self, X, C):\n",
        "        \"\"\"\n",
        "        X:  Real or fake images (Discriminator inputs)\n",
        "        C:  Conditional vector (EEG Features 1D vector concat with noise)\n",
        "        \"\"\"\n",
        "        return self.gen(X), self.disc(X, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3nRI5xrh4vt"
      },
      "outputs": [],
      "source": [
        "def dist_train_step(mirrored_strategy, model, model_gopt, model_copt, X, C, latent_dim=96, batch_size=64):\n",
        "    diff_augment_policies = \"color,translation\"\n",
        "    noise_vector = torch.rand(batch_size, latent_dim, device=X.device) * 2 - 1\n",
        "    noise_vector_2 = torch.rand(batch_size, latent_dim, device=X.device) * 2 - 1\n",
        "    noise_vector = torch.cat([noise_vector, C], dim=-1)\n",
        "    noise_vector_2 = torch.cat([noise_vector_2, C], dim=-1)\n",
        "\n",
        "    def disc_hinge(D_real, D_fake):\n",
        "        return (torch.mean(nn.ReLU()(1 - D_real)) + torch.mean(nn.ReLU()(1 + D_fake))) / 2\n",
        "\n",
        "    def gen_hinge(D_fake):\n",
        "        return -torch.mean(D_fake)\n",
        "\n",
        "    def train_step_disc(model, model_gopt, model_copt, X, C, latent_dim=96, batch_size=64):\n",
        "        model_copt.zero_grad()\n",
        "\n",
        "        fake_img = model.gen(noise_vector)\n",
        "\n",
        "        X_aug = TF.affine(TF.to_pil_image(X), *TF._get_inverse_affine_args(*TF._random_affine(TF.to_tensor(X), **TF.random_affine_params(0, 0.3, 0.1, 0.1, 5, False)), TF.to_pil_image(X).size, interpolation=TF.InterpolationMode.BILINEAR))\n",
        "        X_aug = TF.to_tensor(X_aug).to(X.device)\n",
        "        fake_img = TF.affine(TF.to_pil_image(fake_img), *TF._get_inverse_affine_args(*TF._random_affine(TF.to_tensor(fake_img), **TF.random_affine_params(0, 0.3, 0.1, 0.1, 5, False)), TF.to_pil_image(fake_img).size, interpolation=TF.InterpolationMode.BILINEAR))\n",
        "        fake_img = TF.to_tensor(fake_img).to(X.device)\n",
        "\n",
        "        D_real, X_recon = model.disc(X_aug, C, training=True)\n",
        "        D_fake, _ = model.disc(fake_img, C, training=True)\n",
        "\n",
        "        c_loss = disc_hinge(D_real, D_fake)\n",
        "\n",
        "        c_loss.backward()\n",
        "        model_copt.step()\n",
        "        return c_loss.item()\n",
        "\n",
        "    def train_step_gen(model, model_gopt, model_copt, X, C, latent_dim=96, batch_size=64):\n",
        "        model_gopt.zero_grad()\n",
        "\n",
        "        fake_img_o = model.gen(noise_vector)\n",
        "        fake_img_2_o = model.gen(noise_vector_2)\n",
        "        fake_img = TF.affine(TF.to_pil_image(fake_img_o), *TF._get_inverse_affine_args(*TF._random_affine(TF.to_tensor(fake_img_o), **TF.random_affine_params(0, 0.3, 0.1, 0.1, 5, False)), TF.to_pil_image(fake_img_o).size, interpolation=TF.InterpolationMode.BILINEAR))\n",
        "        fake_img = TF.to_tensor(fake_img).to(X.device)\n",
        "        fake_img_2 = TF.affine(TF.to_pil_image(fake_img_2_o), *TF._get_inverse_affine_args(*TF._random_affine(TF.to_tensor(fake_img_2_o), **TF.random_affine_params(0, 0.3, 0.1, 0.1, 5, False)), TF.to_pil_image(fake_img_2_o).size, interpolation=TF.InterpolationMode.BILINEAR))\n",
        "        fake_img_2 = TF.to_tensor(fake_img_2).to(X.device)\n",
        "\n",
        "        D_fake, _ = model.disc(fake_img, C, training=False)\n",
        "        D_fake_2, _ = model.disc(fake_img_2, C, training=False)\n",
        "        g_loss = gen_hinge(D_fake) + gen_hinge(D_fake_2)\n",
        "        mode_loss = torch.mean(torch.abs(fake_img_2_o - fake_img_o)) / torch.mean(torch.abs(noise_vector_2 - noise_vector))\n",
        "        mode_loss = 1.0 / (mode_loss + 1e-5)\n",
        "        g_loss = g_loss + 1.0 * mode_loss\n",
        "\n",
        "        g_loss.backward()\n",
        "        model_gopt.step()\n",
        "        return g_loss.item()\n",
        "\n",
        "    model.train()\n",
        "    model_gopt = optim.Adam(model.gen.parameters())\n",
        "    model_copt = optim.Adam(model.disc.parameters())\n",
        "    per_replica_loss_disc = mirrored_strategy.run(train_step_disc, args=(model, model_gopt, model_copt, X, C, latent_dim, batch_size,))\n",
        "    per_replica_loss_gen = mirrored_strategy.run(train_step"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMX7d39EzROSRmEZcKdC3ET",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
